---
title: "Semi-synthetic data example: Impact of sparsity"
date: "Compiled at `r format(Sys.time(), '%Y-%m-%d %H:%M:%S', tz = 'UTC')` UTC"
output: github_document
params:
  name: "01-simulations-sparsity" # change if you rename file
---

```{r here, message=FALSE, echo = F}
here::i_am(paste0(params$name, ".Rmd"), uuid = "56755673-5231-49f0-a980-5382fc1275f2")
knitr::opts_chunk$set(dpi = 300, echo = T, warning = F, message = F)
```

```{r directories, echo = F}
# create or *empty* the target directory, used to write this file's data: 
projthis::proj_create_dir_target(params$name, clean = TRUE)
# function to get path to target directory: path_target("sample.csv")
path_target <- projthis::proj_path_target(params$name)
# function to get path to previous data: path_source("00-import", "sample.csv")
path_source <- projthis::proj_path_source(params$name)
```

#### Document overview

In this document we generate a synthetic outcome $y$ based on a combination of linear and interaction effects from real 16s data from the American Gut cohort.  

We vary the position of the true (nonzero) interaction effects and show under which conditions interactions can be identified.

#### Required packages and functions

```{r packages}
library("conflicted")
library(trac)
library(ggplot2)
library(gridExtra)
library(RColorBrewer)
library(reshape2)
```

## 1. Synthetic data generation based on American gut data 

```{r load-scripts}
## load data
## This dataset was processed and aggregated in 
## https://github.com/jacobbien/trac-reproducible.
path.data <- path.data <- "data/"
source("R/sparse_log_contrast.R")
source("R/slc_int.R")
source("R/slc_int_plots.R")
source("R/log-ratio-lasso_interactions.R")
source("R/utils.R")
```

```{r read-data}
dat_list <- readRDS(paste0(path.data, "AGP_aggregated.RDS"))
```

```{r}
## extract OTU level
i = "OTU"
dat_OTU <- dat_list[[i]]
X_OTU <- dat_OTU$x 
y_BMI <- dat_OTU$y
dim(X_OTU)
## subsample 100 samples to no be in the high-dim regime anymore 
set.seed(123)
subs_index <- sample(seq(nrow(X_OTU)), 300)
X_OTU <- X_OTU[subs_index, ]
dim(X_OTU)
```
Let's check how sparse the features are and sort them by sparsity (from dense to sparse).

```{r}
sort_sparstiy <- order(colSums(X_OTU != 0)/nrow(X_OTU), decreasing = T)
X_OTU <- X_OTU[, sort_sparstiy]
colnames(X_OTU) <- paste0("f", 1:ncol(X_OTU))
library(ggplot2)
library(dplyr)
data.frame("Sparsity" = colSums(X_OTU != 0)/nrow(X_OTU),
           "Feature" = factor(colnames(X_OTU), levels = colnames(X_OTU)) ) %>% 
  ggplot(aes(x = Feature, y = Sparsity)) +
  geom_bar(stat="identity") +
  theme_minimal() +
  theme(axis.text.x=element_blank(), 
      axis.ticks.x=element_blank()
    ) +
  xlab("Features") + ylab("% non-zero entries")  
```

For runtime reasons we subsample 50 features features from X.

```{r}
X_OTU_init <- X_OTU
set.seed(123)
X_OTU <- X_OTU_init[, sort(sample(1:ncol(X_OTU_init), 50))]
X_OTU[, 49] <- X_OTU_init[, ncol(X_OTU_init) - 1]
X_OTU[, 50] <- X_OTU_init[, ncol(X_OTU_init)]
## rename again f1,...,f50
colnames(X_OTU) <- paste0("f", 1:50)
```

```{r}
library(RColorBrewer)
set.seed(123)
subsample_viz <- sample(1:nrow(X_OTU), 300)
X_OTU_viz <- X_OTU#[subsample_viz, ]
X_OTU_viz[X_OTU_viz == 0] <- NA
heatmap_OTUs <- pheatmap::pheatmap(log(X_OTU_viz), cluster_cols = F, cluster_rows = F,
                   fontsize_row = 0.0001, na_col = "white", border_color = "white",
                   color = colorRampPalette((brewer.pal(n = 7, name =
  "Greys")))(100), fontsize_col = 11)
```

```{r}
data.frame("Sparsity" = colSums(X_OTU != 0)/nrow(X_OTU),
           "Feature" = factor(colnames(X_OTU), levels = colnames(X_OTU)) ) %>% 
  ggplot(aes(x = Feature, y = Sparsity)) +
  geom_bar(stat="identity") +
  theme_minimal() +
  theme(axis.text.x=element_blank(), 
      axis.ticks.x=element_blank()
    ) +
  xlab("Features") + ylab("% non-zero entries")
```

### Transform data

```{r}
## add pseudo count and build relative abundances
pseudo_count <- 1
X_OTU_psd <- as.matrix(X_OTU + pseudo_count)
X_OTU_rel <- X_OTU_psd/rowSums(X_OTU_psd)
```

```{r}
## compute interactions 
X_OTU_interact <- compute.interactions.aitchison(X_OTU_rel)
```

```{r}
data.frame("Sparsity" = colSums(X_OTU_interact != 0)/nrow(X_OTU_interact),
           "Feature" = factor(colnames(X_OTU_interact), 
                              levels = colnames(X_OTU_interact)) ) %>% 
  ggplot(aes(x = Feature, y = Sparsity)) +
  geom_bar(stat="identity") +
  theme_minimal() +
  theme(axis.text.x=element_blank(), 
      axis.ticks.x=element_blank()
    ) +
  xlab("Features") + ylab("% non-zero entries")
```

### Generate semi-synthetic outcomes

In the first round we are choosing models with three main effects and one interaction effect.
We make sure the the interaction effect is not correlated with the three main effects.
We vary the position of the interaction effects from dense to sparse interaction features.

define interactions

```{r}
int_all <- c("f7:f8",
             "f15:f16",
             "f29:f30", 
             "f39:f40",
             "f49:f50")
```

Sparsity levels for selected interactions

```{r}
df_sparse_int <- data.frame("Sparsity" = colSums(X_OTU_interact != 0)/nrow(X_OTU_interact),
           "Feature" = factor(colnames(X_OTU_interact), 
                              levels = colnames(X_OTU_interact)) )
df_sparse_int[which(df_sparse_int$Feature %in% int_all), ]
```

```{r}
df_countSum_int <- data.frame("CountSum" = colSums(X_OTU_interact),
           "Feature" = factor(colnames(X_OTU_interact), 
                              levels = colnames(X_OTU_interact)) )
df_countSum_int[which(df_countSum_int$Feature %in% int_all), ]
```

```{r}
cor_mat <- cor(cbind(X_OTU_rel[, c(1:3)], X_OTU_interact[, int_all]),
    use = "pairwise.complete.obs", method = "kendall")
cor_mat 
pheatmap::pheatmap(cor_mat, cluster_cols = F, cluster_rows = F,
                   color = colorRampPalette((brewer.pal(n = 7, name =
  "Greys")))(100)[30:100], border_color = "white", display_numbers = T,
  cellwidth = 23, cellheight = 23, number_color = "white",
  fontsize_col = 11, fontsize_row = 11)
```

Mean correlation with main effects of absolute correlation values

```{r}
mean_cor <- numeric(length(int_all))
names(mean_cor) <- int_all
for(i in int_all){
  mean_cor[i] <- round(max(abs(cor_mat[i, 1:3])), 2)
}
mean_cor
```

Effect sizes

```{r}
colSums(abs(X_OTU_interact[, int_all] * 10))
eff_size <- colSums(abs(X_OTU_interact[, int_all]))
```

```{r}
## 3 nonzero main effects
beta_main <-  c(10, 20, -30) 
main_part = log(X_OTU_rel[, c(1:3)]) %*% beta_main
## One nonzero interaction effect
interaction_part <- list()
Y_sim_int <- list()
set.seed(123)
noise <- 10 * rnorm(nrow(X_OTU_rel))
i <- 0
for(m in int_all){
  i <- i + 1
  beta_interact = 3
  interaction_part[[i]] = X_OTU_interact[, m] * beta_interact
  Y_sim_int[[i]] <- 10 + main_part + interaction_part[[i]] + noise
  Y_sim_int[[i]] <- as.numeric(Y_sim_int[[i]])
}
```

```{r eval = T, results='hide'}
n_m <- length(int_all)
slc_slc_int <- list()
for(i in 1:n_m) {
  print(paste("Model:", i))
  slc_slc_int[[i]] <- slc_slc_int_all_splits(X = as.matrix(X_OTU_rel),
                                                             y = Y_sim_int[[i]],
                                                             method = "regr", output = "raw",
                                             nbsplit = 10,
                                             #nbsplit = 5,
                                             ii = "i1se")
}
```

```{r echo = F}
p = 50
mod <- slc_slc_int
  plt_coef <- list()
  for(ii in seq(length(mod))) {
    # Reshape the data to long format using melt
    data_long <- melt(mod[[ii]]$beta_int_est_refit[c("f1", "f2", "f3", int_all[ii]), ])
    plt_coef[[ii]] <- ggplot(data_long, aes(x = as.factor(Var1), y = value)) +
      geom_boxplot(fill = "white", color = "black") +
      geom_point(data = data.frame(x = 1:4, y = c(10, 20, -30, 3)),
               aes(x, y, color = "True Coefficient"),
               pch = 20, size = 4, alpha = .7) +
      labs(
        y = "Estimated coefficients 10 splits",
        title = "",
      ) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 65, hjust = 1, size = 6)) +
      scale_x_discrete(name = "Feature") +
      scale_color_manual(values = c("True Coefficient" = "blue"), name = "Legend") +
      guides(fill = guide_legend(title = "Boxplot"))
}
plt_coef[[1]]
plt_coef[[2]]
plt_coef[[3]]
plt_coef[[4]]
plt_coef[[5]]
```

### How good are the models in terms of squared distances for the different noise levels?

```{r}
## True coefficients
p = ncol(X_OTU)
beta_main_all <- rep(0, p)
beta_main_all[c(1:3)] <- beta_main
beta_main_int_all <- matrix(nrow = p * (p + 1)/2, ncol = n_m)
rownames(beta_main_int_all) <- c(paste0("f", as.character(1:p)), colnames(X_OTU_interact))
for(i in 1:n_m){
  beta_main_int_all[c(paste0("f", as.character(1:3)), int_all[i]), i] <- c(beta_main, beta_interact)
}
beta_main_int_all[is.na(beta_main_int_all)] <- 0
beta_main_int_all[which(rowSums(beta_main_int_all) != 0), ]
```

### True effects are only main effects

```{r fig.width=9}
## Calculate squared distances
nsplit = 10
dist_nf_int <- matrix(nrow = nsplit, ncol = n_m)
for(i in 1:n_m){
  int_i <- int_all[i]
  ## extract beta estimates (1se) for each train-test split
  beta_est_int = slc_slc_int[[i]]$beta_int_est_refit
  #beta_est_int[abs(beta_est_int) < 5] <- 0
  #dist_nf_int[, i] = colSums(sqrt((beta_est_int - beta_main_int_all[, i])^2))
  dist_nf_int[, i] = sqrt((beta_est_int[int_i, ] - beta_main_int_all[int_i, i])^2)
}
colnames(dist_nf_int) <- int_all
```

```{r}
# Assuming dist_nf_int is your matrix
# Convert the matrix to a data frame for ggplot
dist_nf_int_ <- dist_nf_int
colnames(dist_nf_int_) <- paste0(colnames(dist_nf_int),
                                " (", round(1- df_sparse_int[colnames(dist_nf_int), ]$Sparsity, 2), ")")
# Load the ggplot2 library
library(ggplot2)
dist_nf_int.l <- reshape2::melt(dist_nf_int_)
# Create the boxplot using ggplot
plt_esterr_sparsity <- dist_nf_int.l %>% ggplot(aes(x = Var2, y = value)) +
  geom_boxplot() +
  labs(title = "", y = expression("Estimation error"~~sqrt((theta[ "jk" ]^'*' - hat(theta)[ "jk" ])^2)), 
       x = expression("Model with non-zero interaction between feature"~italic("j")*":"*italic("k")~"(f"*italic("j")*":f"*italic("k")~") (sparsity)")
 ) +
  theme_minimal() + 
  theme(axis.text.x = element_text(colour="black"), 
        axis.ticks = element_line(color = "black")) #+ ylim(0, 25) 
plt_esterr_sparsity 
summary(dist_nf_int)
apply(dist_nf_int, 2, var)
apply(dist_nf_int, 2, median)
```

```{r echo = F, eval = F}
for(i in seq(length(int_all))){
  beta_i.long <- reshape2::melt(t(slc_slc_int[[i]]$beta_int_est_refit))
  beta_i.long <- beta_i.long[beta_i.long$Var2 %in% c(c("f1", "f2", "f3"), int_all[i]), ]
  # Create the boxplot using ggplot
  plt_beta_i <- beta_i.long %>% ggplot(aes(x = Var2, y = value)) +
    geom_boxplot() +
    labs(title = "", y = expression(hat(beta) ), x = "True nonzero features" ) +
    theme_minimal() + 
    theme(axis.text.x = element_text(colour="black"), 
          axis.ticks = element_line(color = "black"))
  plt_beta_i
}
```

### path

```{r}
for(i in 1:5){
  plt_path <- ggplot_path(
  slc_int = slc_slc_int[[i]]$fit_int, 
  cvslc_int = slc_slc_int[[i]]$cvfit_int, 
  p = ncol(X_OTU),
  main = paste0("Model s=", i), r = 1,
  colnames_main_nz = paste0("f", as.character(1:3)),
  colnames_int_nz =  int_all[i], 
  feature_names = rownames(slc_slc_int[[i]]$beta_int_est_refit))
  if(i == 1){
  print(plt_path)
  }
}
```

## Synthetic scenarios with more main + interaction effects

### OTU level

```{r}
X_OTU <- dat_OTU$x 
sort_sparstiy <- order(colSums(X_OTU != 0)/nrow(X_OTU), decreasing = T)
X_OTU <- X_OTU[, sort_sparstiy]
colnames(X_OTU) <- paste0("f", 1:ncol(X_OTU))
```

For runtime reasons we subsample 50 features features from X.

```{r}
X_OTU_init <- X_OTU
set.seed(123)
X_OTU <- X_OTU_init[, sort(sample(1:ncol(X_OTU_init), 50))]
X_OTU[, 49] <- X_OTU_init[, ncol(X_OTU_init) - 1]
X_OTU[, 50] <- X_OTU_init[, ncol(X_OTU_init)]
## rename again f1,...,f50
colnames(X_OTU) <- paste0("f", 1:50)
```

### Transform data

```{r}
## add pseudo count and build relative abundances
pseudo_count <- 1
X_OTU_psd <- as.matrix(X_OTU + pseudo_count)
X_OTU_rel <- X_OTU_psd/rowSums(X_OTU_psd)
```

```{r}
## compute interactions 
X_OTU_interact <- compute.interactions.aitchison(X_OTU_rel)
```

```{r}
## 3 nonzero main effects
set.seed(123)
beta_main <-  rnorm(15, mean = 0, sd = 20)
main_part = log(X_OTU_rel[, c(1:15)]) %*% beta_main
## One nonzero interaction effect
interaction_part <- list()
Y_sim_int <- list()
set.seed(123)
noise <- 10 * rnorm(nrow(X_OTU_rel))
int_all <- c(1:10)
set.seed(123)
beta_interact = rnorm(10, mean = 0, sd = 20)
i=1
interaction_part[[i]] = X_OTU_interact[, 1:10] %*% beta_interact
Y_sim_int[[i]] <- 10 + main_part + interaction_part[[i]] + noise
Y_sim_int[[i]] <- as.numeric(Y_sim_int[[i]])
```

```{r eval = T, results='hide'}
slc_slc_int <- list()
for(i in 1) {
  print(paste("Model:", i))
  slc_slc_int[[i]] <- slc_slc_int_all_splits(X = as.matrix(X_OTU_rel),
                                                             y = Y_sim_int[[i]],
                                                             method = "regr", output = "raw",
                                             nbsplit = 10,
                                             #nbsplit = 5,
                                             ii = "i1se")
}
```

```{r}
library(ggplot2)
mod <- slc_slc_int
# Prepare data
predicted <- rowMeans(mod[[1]]$beta_int_est_refit)
observed <- c(beta_main, rep(0, 35), beta_interact, rep(0, 1275 - 60))
# Create a group variable
group <- ifelse(1:length(predicted) <= 50, "Main effects", "Interaction effects")
data <- data.frame(predicted = predicted, observed = observed, group = group)
# Create ggplot
ggplot(data, aes(x = predicted, y = observed, color = group)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  scale_color_manual(values = c("Main effects" = "lightblue", "Interaction effects" = "darkblue")) +
  labs(x = "Predicted", y = "Observed", title = "Predicted vs Observed", color = "Group") +
  theme_minimal()
```

### Regularization path

```{r}
i = 1  
nam = rownames(slc_slc_int[[i]]$beta_int_est_refit)
plt_path <- ggplot_path2(
  slc_int = slc_slc_int[[i]]$fit_int, 
  cvslc_int = slc_slc_int[[i]]$cvfit_int, 
  p = ncol(X_OTU),
  #main = paste0("Model s=", i), r = 1,
  colnames_main_nz = paste0("f", as.character(1:30)),
  colnames_int_nz =  nam[51:length(nam)][int_all], 
  feature_names = nam
)
```

### Family level

```{r}
i = "Family"
dat_Family <- dat_list[[i]]
X_Family <- dat_Family$x
## 50 most abundant families
abund_fam <- colSums(X_Family != 0)
abund_fam <- names(sort(abund_fam))
X_Family <- X_Family[, abund_fam[1:50]]
y_BMI <- dat_Family$y
colnames(X_Family) <- paste0("f", 1:ncol(X_Family))
dim(X_Family)
```

### Transform data

```{r}
## add pseudo count and build relative abundances
pseudo_count <- 1
X_Family_psd <- as.matrix(X_Family + pseudo_count)
X_Family_rel <- X_Family_psd/rowSums(X_Family_psd)
```

```{r}
## compute interactions 
X_Family_interact <- compute.interactions.aitchison(X_Family_rel)
```

### 1. 15 main effects, 20 interaction effects

```{r}
## nonzero main effects
set.seed(123)
pnz <- 15
beta_main <-  rnorm(pnz, mean = 0, sd = 20)
main_part = log(X_Family_rel[, c(1:pnz)]) %*% beta_main
## One nonzero interaction effect
interaction_part <- list()
Y_sim_int <- list()
set.seed(123)
noise <-  rnorm(nrow(X_Family_rel))
int_all <- c(1:20)
set.seed(123)
beta_interact = rnorm(length(int_all), mean = 0, sd = 20)
i=1
interaction_part[[i]] = X_Family_interact[, 1:length(int_all)] %*% beta_interact
Y_sim_int[[i]] <- 10 + main_part + interaction_part[[i]] + noise
Y_sim_int[[i]] <- as.numeric(Y_sim_int[[i]])
```

```{r eval = T, results='hide'}
slc_slc_int1 <- list()
i = 1
slc_slc_int1[[i]] <- slc_slc_int_all_splits(X = as.matrix(X_Family_rel),
                                           y = Y_sim_int[[i]],
                                           method = "regr", output = "raw",
                                           nbsplit = 10,
                                           #nbsplit = 5,
                                           ii = "i1se")
```

```{r}
library(ggplot2)
mod <- slc_slc_int1
# Prepare data
## mean over 10 train-test splits
beta_hat <- rowMeans(mod[[1]]$beta_int_est_refit)
beta_true <- c(beta_main, rep(0, ncol(X_Family) - pnz), beta_interact, 
               rep(0, ncol(X_Family_interact) - length(int_all)))
# Create a group variable
group <- ifelse(1:length(beta_hat) <= 10, "Main effects", "Interaction effects")
data <- data.frame(beta_hat = beta_hat, beta_true = beta_true, group = group)
estim_err <- mod[[1]]$beta_int_est_refit - beta_true
(mean_abs_estim_err <- mean(colMeans(abs(estim_err))))
mean(colMeans(abs(mod[[1]]$beta_int_est_refit)))
# Create ggplot
scatterplt <- ggplot(data, aes(x = beta_hat, y = beta_true, color = group)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  scale_color_manual(values = c("Main effects" = "lightblue", "Interaction effects" = "darkred")) +
  labs(x = "Mean estimates (10 train-test splits)", y = "True coefficient", title = "", color = "Group") +
  theme_minimal() + 
  theme(axis.text.x = element_text(colour = "black", size = 12),
        axis.ticks.x = element_blank(),
        axis.text.y = element_text(colour = "black", size = 12),
        axis.ticks.y = element_line(colour = "black"))
# ggsave(paste0(path, "sim_suppl_1_scatter.pdf"), scatterplt, height = 3.5, width = 4.5)
```

### R squared 

```{r echo = F}
train_mse_slc <- c()
test_mse_slc <- c()
train_mse_slc_int <- c()
test_mse_slc_int <- c()
r_squared_train_slc <- c()
r_squared_test_slc <- c()
r_squared_train_slc_int <- c()
r_squared_test_slc_int <- c()
ii <- 1
# n <- length(Y_sim_phylum_int[[ii]]) - length(mod[[ii]]$tr[[r]])
# ntot <- length(Y_sim_phylum_int[[ii]])
y <- Y_sim_int[[1]]
for(r in seq(nsplit)){
  yhat_trbest <- mod[[ii]]$yhat_tr[[r]][, mod[[ii]]$cvfit[[r]]$cv$ibest]
  yhat_tebest <- mod[[ii]]$yhat_te[[r]][, mod[[ii]]$cvfit[[r]]$cv$ibest]
  yhat_tr_intbest <- mod[[ii]]$yhat_tr_int[[r]][, mod[[ii]]$cvfit[[r]]$cv$ibest]
  yhat_te_intbest <- mod[[ii]]$yhat_te_int[[r]][, mod[[ii]]$cvfit[[r]]$cv$ibest]
  ytr <- y[mod[[ii]]$tr[[r]]]
  yte <- y[-mod[[ii]]$tr[[r]]]
  train_mse_slc[r] <- (mean((yhat_trbest - ytr)^2))
  test_mse_slc[r] <- (mean((yhat_tebest - yte)^2))
  train_mse_slc_int[r] <- (mean((yhat_tr_intbest - ytr)^2))
  test_mse_slc_int[r] <- (mean((yhat_te_intbest - yte)^2))
  observed_values <- ytr
  predicted_values <- yhat_trbest
  # Calculate mean of observed values
  mean_observed <- mean(observed_values)
  # Calculate R-squared
  ss_residual <- sum((observed_values - predicted_values)^2)
  ss_total <- sum((observed_values - mean_observed)^2)
  r_squared <- 1 - (ss_residual / ss_total)
  n <- length(ytr)
  r_squared_train_slc[r] <- r_squared
  observed_values <- yte
  predicted_values <- yhat_tebest
  # Calculate mean of observed values
  mean_observed <- mean(observed_values)
  # Calculate R-squared
  ss_residual <- sum((observed_values - predicted_values)^2)
  ss_total <- sum((observed_values - mean_observed)^2)
  r_squared <- 1 - (ss_residual / ss_total)
  n <- length(yte)
  r_squared_test_slc[r] <- r_squared
  observed_values <- ytr
  predicted_values <- yhat_tr_intbest
  # Calculate mean of observed values
  mean_observed <- mean(observed_values)
  # Calculate R-squared
  ss_residual <- sum((observed_values - predicted_values)^2)
  ss_total <- sum((observed_values - mean_observed)^2)
  r_squared <- 1 - (ss_residual / ss_total)
  n <- length(ytr)
  r_squared_train_slc_int[r] <- r_squared
  observed_values <- yte
  predicted_values <- yhat_te_intbest
  # Calculate mean of observed values
  mean_observed <- mean(observed_values)
  # Calculate R-squared
  ss_residual <- sum((observed_values - predicted_values)^2)
  ss_total <- sum((observed_values - mean_observed)^2)
  r_squared <- 1 - (ss_residual / ss_total)
  n <- length(yte)
  r_squared_test_slc_int[r] <- r_squared
}
df <- data.frame(
  r_squared = c(r_squared_train_slc, r_squared_test_slc,
                    r_squared_train_slc_int, r_squared_test_slc_int),
  group = rep(c("slc train", "slc test", "sparse qlc train", "sparse qlc test"), each = nsplit),
  model = rep(c("Main effects model (sparse lc)", "Interaction model (sparse qlc)"), each = nsplit * 2),
  traintest = rep(c("train", "test", "train", "test"), each = nsplit))
# Add information about the underlying sample size to the group names
df$group <- factor(df$group, levels =c("slc train", "slc test", "sparse qlc train", "sparse qlc test") )
df$model <- factor(df$model, levels =c("Main effects model (sparse lc)", "Interaction model (sparse qlc)") )
df$traintest <- factor(df$traintest, levels = c("train", "test"))
#remotes::install_github("coolbutuseless/ggpattern")
library(ggpattern)
# Create the boxplot
plt <- ggplot(df, aes(x = group, y = r_squared, fill = model, pattern = traintest)) +
  geom_boxplot() +
  scale_fill_manual(values = c("lightblue","darkred")) +
  # scale_color_manual(values=c("steelblue", "azure3")) + 
  geom_boxplot_pattern(position = position_dodge(preserve = "single"), color = "black", pattern_fill = "white",
                       pattern_angle = 45, pattern_density = 0.1, pattern_spacing = 0.025, 
                       pattern_key_scale_factor = 0.6) +
  scale_pattern_manual(values = c(test = "stripe", train  = "none")) +
  labs(x = "", y = expression("R"^2)) +
  theme_minimal() +
  theme(axis.text.x = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_text(colour = "black", size = 12)) +
  guides(
    fill = guide_legend(override.aes = list(pattern = "none"), title = "Model"),
         pattern = guide_legend(override.aes = list(fill = "white"), title = "Train or test data"))
print(plt)
# ggsave(paste0(path, "sim_suppl_1.pdf"), plt, height = 4, width = 5)
```
### 2. 30 main effects, 40 interactions

```{r}
## nonzero main effects
set.seed(123)
pnz <- 30
beta_main <-  rnorm(pnz, mean = 0, sd = 20)
main_part = log(X_Family_rel[, c(1:pnz)]) %*% beta_main
## One nonzero interaction effect
interaction_part <- list()
Y_sim_int <- list()
set.seed(123)
noise <-  rnorm(nrow(X_Family_rel))
int_all <- c(1:40)
set.seed(123)
beta_interact = rnorm(length(int_all), mean = 0, sd = 20)
i=1
interaction_part[[i]] = X_Family_interact[, 1:length(int_all)] %*% beta_interact
Y_sim_int[[i]] <- 10 + main_part + interaction_part[[i]] + noise
Y_sim_int[[i]] <- as.numeric(Y_sim_int[[i]])
```

```{r eval = T, results='hide'}
slc_slc_int2 <- list()
for(i in 1) {
  print(paste("Model:", i))
  slc_slc_int2[[i]] <- slc_slc_int_all_splits(X = as.matrix(X_Family_rel),
                                                             y = Y_sim_int[[i]],
                                                             method = "regr", output = "raw",
                                             nbsplit = 10,
                                             #nbsplit = 5,
                                             ii = "i1se")
}
```

```{r}
library(ggplot2)
mod <- slc_slc_int2
# Prepare data
## mean over 10 train-test splits
beta_hat <- rowMeans(mod[[1]]$beta_int_est_refit)
beta_true <- c(beta_main, rep(0, ncol(X_Family) - pnz), beta_interact, 
               rep(0, ncol(X_Family_interact) - length(int_all)))
# Create a group variable
group <- ifelse(1:length(beta_hat) <= 10, "Main effects", "Interaction effects")
data <- data.frame(beta_hat = beta_hat, beta_true = beta_true, group = group)
estim_err <- mod[[1]]$beta_int_est_refit - beta_true
(mean_abs_estim_err <- mean(colMeans(abs(estim_err))))
mean(colMeans(abs(mod[[1]]$beta_int_est_refit)))
# Create ggplot
scatterplt <- ggplot(data, aes(x = beta_hat, y = beta_true, color = group)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  scale_color_manual(values = c("Main effects" = "lightblue", "Interaction effects" = "darkred")) +
  labs(x = "Mean estimates (10 train-test splits)", y = "True coefficient", title = "", color = "Group") +
  theme_minimal() + 
  theme(axis.text.x = element_text(colour = "black", size = 12),
        axis.ticks.x = element_blank(),
        axis.text.y = element_text(colour = "black", size = 12),
        axis.ticks.y = element_line(colour = "black"))
# ggsave(paste0(path, "sim_suppl_2_scatter.pdf"), scatterplt, height = 3.4, width = 4.5)
```

### R squared 

```{r echo = F}
train_mse_slc <- c()
test_mse_slc <- c()
train_mse_slc_int <- c()
test_mse_slc_int <- c()
r_squared_train_slc <- c()
r_squared_test_slc <- c()
r_squared_train_slc_int <- c()
r_squared_test_slc_int <- c()
ii <- 1
# n <- length(Y_sim_phylum_int[[ii]]) - length(mod[[ii]]$tr[[r]])
# ntot <- length(Y_sim_phylum_int[[ii]])
y <- Y_sim_int[[1]]
for(r in seq(nsplit)){
  yhat_trbest <- mod[[ii]]$yhat_tr[[r]][, mod[[ii]]$cvfit[[r]]$cv$ibest]
  yhat_tebest <- mod[[ii]]$yhat_te[[r]][, mod[[ii]]$cvfit[[r]]$cv$ibest]
  yhat_tr_intbest <- mod[[ii]]$yhat_tr_int[[r]][, mod[[ii]]$cvfit[[r]]$cv$ibest]
  yhat_te_intbest <- mod[[ii]]$yhat_te_int[[r]][, mod[[ii]]$cvfit[[r]]$cv$ibest]
  ytr <- y[mod[[ii]]$tr[[r]]]
  yte <- y[-mod[[ii]]$tr[[r]]]
  train_mse_slc[r] <- (mean((yhat_trbest - ytr)^2))
  test_mse_slc[r] <- (mean((yhat_tebest - yte)^2))
  train_mse_slc_int[r] <- (mean((yhat_tr_intbest - ytr)^2))
  test_mse_slc_int[r] <- (mean((yhat_te_intbest - yte)^2))
  observed_values <- ytr
  predicted_values <- yhat_trbest
  # Calculate mean of observed values
  mean_observed <- mean(observed_values)
  # Calculate R-squared
  ss_residual <- sum((observed_values - predicted_values)^2)
  ss_total <- sum((observed_values - mean_observed)^2)
  r_squared <- 1 - (ss_residual / ss_total)
  n <- length(ytr)
  r_squared_train_slc[r] <- r_squared
  observed_values <- yte
  predicted_values <- yhat_tebest
  # Calculate mean of observed values
  mean_observed <- mean(observed_values)
  # Calculate R-squared
  ss_residual <- sum((observed_values - predicted_values)^2)
  ss_total <- sum((observed_values - mean_observed)^2)
  r_squared <- 1 - (ss_residual / ss_total)
  n <- length(yte)
  r_squared_test_slc[r] <- r_squared
  observed_values <- ytr
  predicted_values <- yhat_tr_intbest
  # Calculate mean of observed values
  mean_observed <- mean(observed_values)
  # Calculate R-squared
  ss_residual <- sum((observed_values - predicted_values)^2)
  ss_total <- sum((observed_values - mean_observed)^2)
  r_squared <- 1 - (ss_residual / ss_total)
  n <- length(ytr)
  r_squared_train_slc_int[r] <- r_squared
  observed_values <- yte
  predicted_values <- yhat_te_intbest
  # Calculate mean of observed values
  mean_observed <- mean(observed_values)
  # Calculate R-squared
  ss_residual <- sum((observed_values - predicted_values)^2)
  ss_total <- sum((observed_values - mean_observed)^2)
  r_squared <- 1 - (ss_residual / ss_total)
  n <- length(yte)
  r_squared_test_slc_int[r] <- r_squared
}
df <- data.frame(
  r_squared = c(r_squared_train_slc, r_squared_test_slc,
                    r_squared_train_slc_int, r_squared_test_slc_int),
  group = rep(c("slc train", "slc test", "sparse qlc train", "sparse qlc test"), each = nsplit),
  model = rep(c("Main effects model (sparse lc)", "Interaction model (sparse qlc)"), each = nsplit * 2),
  traintest = rep(c("train", "test", "train", "test"), each = nsplit))
# Add information about the underlying sample size to the group names
df$group <- factor(df$group, levels =c("slc train", "slc test", "sparse qlc train", "sparse qlc test") )
df$model <- factor(df$model, levels =c("Main effects model (sparse lc)", "Interaction model (sparse qlc)") )
df$traintest <- factor(df$traintest, levels = c("train", "test"))
#remotes::install_github("coolbutuseless/ggpattern")
library(ggpattern)
# Create the boxplot
plt <- ggplot(df, aes(x = group, y = r_squared, fill = model, pattern = traintest)) +
  geom_boxplot() +
  scale_fill_manual(values = c("lightblue3","darkred")) +
  # scale_color_manual(values=c("steelblue", "azure3")) + 
  geom_boxplot_pattern(position = position_dodge(preserve = "single"), color = "black", pattern_fill = "white",
                       pattern_angle = 45, pattern_density = 0.1, pattern_spacing = 0.025, 
                       pattern_key_scale_factor = 0.6) +
  scale_pattern_manual(values = c(test = "stripe", train  = "none")) +
  labs(x = "", y = expression("R"^2)) +
  theme_minimal() +
  theme(axis.text.x = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_text(colour = "black", size = 12)) +
  guides(
    fill = guide_legend(override.aes = list(pattern = "none"), title = "Model"),
         pattern = guide_legend(override.aes = list(fill = "white"), title = "Train or test data"))
print(plt)
# ggsave(paste0(path, "sim_suppl_2.pdf"), plt, height = 3.5, width = 5)
```

### 2. 40 main effects, 60 interactions

```{r}
## nonzero main effects
set.seed(123)
pnz <- 40
beta_main <-  rnorm(pnz, mean = 0, sd = 20)
main_part = log(X_Family_rel[, c(1:pnz)]) %*% beta_main
## One nonzero interaction effect
interaction_part <- list()
Y_sim_int <- list()
set.seed(123)
noise <-  rnorm(nrow(X_Family_rel))
int_all <- c(1:60)
set.seed(123)
beta_interact = rnorm(length(int_all), mean = 0, sd = 20)
i=1
interaction_part[[i]] = X_Family_interact[, 1:length(int_all)] %*% beta_interact
Y_sim_int[[i]] <- 10 + main_part + interaction_part[[i]] + noise
Y_sim_int[[i]] <- as.numeric(Y_sim_int[[i]])
```

```{r eval = T, results='hide'}
slc_slc_int3 <- list()
for(i in 1) {
  print(paste("Model:", i))
  slc_slc_int3[[i]] <- slc_slc_int_all_splits(X = as.matrix(X_Family_rel),
                                                             y = Y_sim_int[[i]],
                                                             method = "regr", output = "raw",
                                             nbsplit = 10,
                                             #nbsplit = 5,
                                             ii = "i1se")
}
```

```{r}
library(ggplot2)
mod <- slc_slc_int3
# Prepare data
## mean over 10 train-test splits
beta_hat <- rowMeans(mod[[1]]$beta_int_est_refit)
beta_true <- c(beta_main, rep(0, ncol(X_Family) - pnz), beta_interact, 
               rep(0, ncol(X_Family_interact) - length(int_all)))
# Create a group variable
group <- ifelse(1:length(beta_hat) <= 10, "Main effects", "Interaction effects")
data <- data.frame(beta_hat = beta_hat, beta_true = beta_true, group = group)
estim_err <- mod[[1]]$beta_int_est_refit - beta_true
(mean_abs_estim_err <- mean(colMeans(abs(estim_err))))
mean(colMeans(abs(mod[[1]]$beta_int_est_refit)))
# Create ggplot
scatterplt <- ggplot(data, aes(x = beta_hat, y = beta_true, color = group)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  scale_color_manual(values = c("Main effects" = "lightblue", "Interaction effects" = "darkred")) +
  labs(x = "Mean estimates (10 train-test splits)", y = "True coefficient", title = "", color = "Group") +
  theme_minimal() + 
  theme(axis.text.x = element_text(colour = "black", size = 12),
        axis.ticks.x = element_blank(),
        axis.text.y = element_text(colour = "black", size = 12),
        axis.ticks.y = element_line(colour = "black"))
# ggsave(paste0(path, "sim_suppl_3_scatter.pdf"), scatterplt, height = 3.5, width = 4.5)
```

### R squared 

```{r echo = F}
train_mse_slc <- c()
test_mse_slc <- c()
train_mse_slc_int <- c()
test_mse_slc_int <- c()
r_squared_train_slc <- c()
r_squared_test_slc <- c()
r_squared_train_slc_int <- c()
r_squared_test_slc_int <- c()
ii <- 1
# n <- length(Y_sim_phylum_int[[ii]]) - length(mod[[ii]]$tr[[r]])
# ntot <- length(Y_sim_phylum_int[[ii]])
y <- Y_sim_int[[1]]
for(r in seq(nsplit)){
  yhat_trbest <- mod[[ii]]$yhat_tr[[r]][, mod[[ii]]$cvfit[[r]]$cv$ibest]
  yhat_tebest <- mod[[ii]]$yhat_te[[r]][, mod[[ii]]$cvfit[[r]]$cv$ibest]
  yhat_tr_intbest <- mod[[ii]]$yhat_tr_int[[r]][, mod[[ii]]$cvfit[[r]]$cv$ibest]
  yhat_te_intbest <- mod[[ii]]$yhat_te_int[[r]][, mod[[ii]]$cvfit[[r]]$cv$ibest]
  ytr <- y[mod[[ii]]$tr[[r]]]
  yte <- y[-mod[[ii]]$tr[[r]]]
  train_mse_slc[r] <- (mean((yhat_trbest - ytr)^2))
  test_mse_slc[r] <- (mean((yhat_tebest - yte)^2))
  train_mse_slc_int[r] <- (mean((yhat_tr_intbest - ytr)^2))
  test_mse_slc_int[r] <- (mean((yhat_te_intbest - yte)^2))
  observed_values <- ytr
  predicted_values <- yhat_trbest
  # Calculate mean of observed values
  mean_observed <- mean(observed_values)
  # Calculate R-squared
  ss_residual <- sum((observed_values - predicted_values)^2)
  ss_total <- sum((observed_values - mean_observed)^2)
  r_squared <- 1 - (ss_residual / ss_total)
  n <- length(ytr)
  r_squared_train_slc[r] <- r_squared
  observed_values <- yte
  predicted_values <- yhat_tebest
  # Calculate mean of observed values
  mean_observed <- mean(observed_values)
  # Calculate R-squared
  ss_residual <- sum((observed_values - predicted_values)^2)
  ss_total <- sum((observed_values - mean_observed)^2)
  r_squared <- 1 - (ss_residual / ss_total)
  n <- length(yte)
  r_squared_test_slc[r] <- r_squared
  observed_values <- ytr
  predicted_values <- yhat_tr_intbest
  # Calculate mean of observed values
  mean_observed <- mean(observed_values)
  # Calculate R-squared
  ss_residual <- sum((observed_values - predicted_values)^2)
  ss_total <- sum((observed_values - mean_observed)^2)
  r_squared <- 1 - (ss_residual / ss_total)
  n <- length(ytr)
  r_squared_train_slc_int[r] <- r_squared
  observed_values <- yte
  predicted_values <- yhat_te_intbest
  # Calculate mean of observed values
  mean_observed <- mean(observed_values)
  # Calculate R-squared
  ss_residual <- sum((observed_values - predicted_values)^2)
  ss_total <- sum((observed_values - mean_observed)^2)
  r_squared <- 1 - (ss_residual / ss_total)
  n <- length(yte)
  r_squared_test_slc_int[r] <- r_squared
}
df <- data.frame(
  r_squared = c(r_squared_train_slc, r_squared_test_slc,
                    r_squared_train_slc_int, r_squared_test_slc_int),
  group = rep(c("slc train", "slc test", "sparse qlc train", "sparse qlc test"), each = nsplit),
  model = rep(c("Main effects model (sparse lc)", "Interaction model (sparse qlc)"), each = nsplit * 2),
  traintest = rep(c("train", "test", "train", "test"), each = nsplit))
# Add information about the underlying sample size to the group names
df$group <- factor(df$group, levels =c("slc train", "slc test", "sparse qlc train", "sparse qlc test") )
df$model <- factor(df$model, levels =c("Main effects model (sparse lc)", "Interaction model (sparse qlc)") )
df$traintest <- factor(df$traintest, levels = c("train", "test"))
#remotes::install_github("coolbutuseless/ggpattern")
library(ggpattern)
# Create the boxplot
plt <- ggplot(df, aes(x = group, y = r_squared, fill = model, pattern = traintest)) +
  geom_boxplot() +
  scale_fill_manual(values = c("lightblue","darkred")) +
  # scale_color_manual(values=c("steelblue", "azure3")) + 
  geom_boxplot_pattern(position = position_dodge(preserve = "single"), color = "black", pattern_fill = "white",
                       pattern_angle = 45, pattern_density = 0.1, pattern_spacing = 0.025, 
                       pattern_key_scale_factor = 0.6) +
  scale_pattern_manual(values = c(test = "stripe", train  = "none")) +
  labs(x = "", y = expression("R"^2)) +
  theme_minimal() +
  theme(axis.text.x = element_blank(),
        axis.ticks = element_blank(),
        axis.text.y = element_text(colour = "black", size = 12)) +
  guides(
    fill = guide_legend(override.aes = list(pattern = "none"), title = "Model"),
         pattern = guide_legend(override.aes = list(fill = "white"), title = "Train or test data"))
print(plt)
# ggsave(paste0(path, "sim_suppl_3.pdf"), plt, height = 3.5, width = 5)
```

## Files written

These files have been written to the target directory, ```r paste0("data/", params$name)```:

```{r list-files-target}
projthis::proj_dir_info(path_target())
```
